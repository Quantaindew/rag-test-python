{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C97yrD9ulH5O"
      },
      "source": [
        "Install packages for [RAG cookbook example](https://python.langchain.com/docs/expression_language/cookbook/retrieval) and [SVM](https://python.langchain.com/docs/integrations/retrievers/svm)(Support Vector Machines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2nIy7RUkKAv",
        "outputId": "31b0f59c-4af4-4790-89c7-0193b097ede7"
      },
      "outputs": [],
      "source": [
        "%pip install langchain openai faiss-cpu tiktoken scikit-learn lark python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "75_ZhDdrmC6l"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "import openai\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import SVMRetriever\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()  # take environment variables from .env."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lxYGp4J3mF_X"
      },
      "outputs": [],
      "source": [
        "retriever = SVMRetriever.from_texts(\n",
        "    [\"harrison worked at kensho\", \"He is 6'4 and 20 years old\"], OpenAIEmbeddings()\n",
        ")\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8a7OftywoYYS"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kNKyeNXvoTib",
        "outputId": "3c78ba14-b4b8-4760-f731-86760215da86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kdsun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Harrison is 6'4 and 20 years old.\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"Tell me something about Harrison\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SBFczLIvHe8"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = {\n",
        "    \"context\": itemgetter(\"question\") | retriever,\n",
        "    \"question\": itemgetter(\"question\"),\n",
        "    \"language\": itemgetter(\"language\")\n",
        "} | prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cxiYcisKvJ9U",
        "outputId": "63e25874-66d2-43ee-97f6-a3d1373981da"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\"question\": \"where did harrison work\", \"language\": \"french\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U279TJFvPP2"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnableMap\n",
        "from langchain.schema import format_document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEdFm8FvvaYo"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4ARYuuIvdhH"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzi9DXbpveG-"
      },
      "outputs": [],
      "source": [
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "def _combine_documents(docs, document_prompt = DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXZJ7CSOvgrd"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
        "    buffer = \"\"\n",
        "    for dialogue_turn in chat_history:\n",
        "        human = \"Human: \" + dialogue_turn[0]\n",
        "        ai = \"Assistant: \" + dialogue_turn[1]\n",
        "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
        "    return buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNom3rirvmdA"
      },
      "outputs": [],
      "source": [
        "_inputs = RunnableMap(\n",
        "    standalone_question=RunnablePassthrough.assign(\n",
        "        chat_history=lambda x: _format_chat_history(x['chat_history'])\n",
        "    ) | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser(),\n",
        ")\n",
        "_context = {\n",
        "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
        "    \"question\": lambda x: x[\"standalone_question\"]\n",
        "}\n",
        "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4Xh3QIPvn-2",
        "outputId": "cd1303ea-1440-475d-afad-d03754f70fdb"
      },
      "outputs": [],
      "source": [
        "conversational_qa_chain.invoke({\n",
        "    \"question\": \"where did harrison work?\",\n",
        "    \"chat_history\": [],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo4IGtZJvqjg"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnCuYgt1v83Y"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo91MiUNvzLa"
      },
      "outputs": [],
      "source": [
        "# First we add a step to load memory\n",
        "# This adds a \"memory\" key to the input object\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "# Now we calculate the standalone question\n",
        "standalone_question = {\n",
        "    \"standalone_question\": {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"chat_history\": lambda x: _format_chat_history(x['chat_history'])\n",
        "    } | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser(),\n",
        "}\n",
        "# Now we retrieve the documents\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"question\": lambda x: x[\"standalone_question\"]\n",
        "}\n",
        "# Now we construct the inputs for the final prompt\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"question\": itemgetter(\"question\")\n",
        "}\n",
        "# And finally, we do the part that returns the answers\n",
        "answer = {\n",
        "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
        "    \"docs\": itemgetter(\"docs\"),\n",
        "}\n",
        "# And now we put it all together!\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOMr14fuv9wf",
        "outputId": "8b5e9ead-6143-48e7-b134-b6cf06c44543"
      },
      "outputs": [],
      "source": [
        "inputs = {\"question\": \"where did harrison work?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0S7_luKwADo"
      },
      "outputs": [],
      "source": [
        "# Note that the memory does not save automatically\n",
        "# This will be improved in the future\n",
        "# For now you need to save it yourself\n",
        "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8pIQBVjwGOV",
        "outputId": "95b4bf41-6c6d-4c0a-abae-cb0c1c58440f"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c8gcbzFwIYv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
